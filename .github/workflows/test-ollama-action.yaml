name: Test Ollama Action
on:
    workflow_dispatch:

jobs:
    test-ollama-action:
        runs-on: ubuntu-latest
        steps:
            - name: Checkout repository
              uses: actions/checkout@v5

            # 一旦コメントアウト
            # - name: Run model
            #   id: run-model
            #   uses: ai-action/ollama-action@v1
            #   with:
            #       model: qwen3:8b
            #       prompt: |
            #           こんにちは、あなたの名前は何ですか？

            # - name: Show output
            #   run: 'echo "Model output: [$response]"'
            #   env:
            #       response: "${{ steps.run-model.outputs.response }}"

            - name: Run Ollama container
              run: |
                  docker run -d --name ollama-server \
                    -v ollama:/root/.ollama \
                    ${IMAGE_NAME}

                  # サーバーが起動するまで待つ
                  echo "Waiting for Ollama server to start..."
                  sleep 5
              env:
                  IMAGE_NAME: ghcr.io/kounoike/too-many-backseat-gamers/ollama-qwen3-0.6b:latest

            - name: Run model
              id: run-model
              run: |
                  docker exec ollama-server ollama run qwen3:0.6b "日本で一番高い山は何でしょう？" > output.txt
                  echo "---------------------結果---------------------"
                  cat output.txt
                  echo "output<<EOF" >> "$GITHUB_OUTPUT"
                  cat output.txt >> "$GITHUB_OUTPUT"
                  echo "EOF" >> "$GITHUB_OUTPUT"

            - name: Cleanup
              if: always()
              run: docker rm -f ollama-server || true
